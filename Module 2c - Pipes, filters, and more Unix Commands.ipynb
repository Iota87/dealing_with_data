{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redirection\n",
    "-----------\n",
    "\n",
    "### The `>` operator\n",
    "\n",
    "A very important command-line operator is the “redirection” operator “`>`”.  With “`>`” you can send the result of your command-line processing to a file.  So if you’re using curl to get your current location, using the ip-api.com service (see the previous section) and want to store the output into a file, you can create a new file with just these lines using redirection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!curl 'http://www.telize.com/geoip' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!curl -s 'http://www.telize.com/geoip' > location.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!curl -s 'http://www.telize.com/geoip' -o location.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls -lA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the content of the file, we can use the command `cat` (described below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "executionInfo": {
     "content": {
      "execution_count": 1,
      "payload": [],
      "status": "ok",
      "user_expressions": {},
      "user_variables": {}
     },
     "timestamp": 1409762051553,
     "user": {
      "color": "#1FA15D",
      "displayName": "Panos Ipeirotis",
      "isAnonymous": false,
      "isMe": true,
      "photoUrl": "//lh4.googleusercontent.com/-dIWj8iHQSKU/AAAAAAAAAAI/AAAAAAAA0Ro/MROYPWvY51A/s50-c-k-no/photo.jpg",
      "sessionId": "67a2c480bd9c6290",
      "userId": "103666871486129948108"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "!cat location.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `>>` operator\n",
    "\n",
    "If we want to append to a file (instead of creating a new file from scratch), then we can use the `>>` operator. The operator is useful in cases where we want to collect data over time (e.g., by setting up a script that runs every hour, and appends the data in the file, instead of overwriting what is there)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!curl 'http://www.telize.com/geoip' > alldata.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat alldata.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!curl \"http://api.openweathermap.org/data/2.5/weather?lat=40.72&lon=-73.98&units=imperial&mode=json\" >> alldata.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat alldata.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!echo 'Open Weather Map Data! Haha!' >>alldata.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat alldata.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "* Let's get now the \"Restaurant Inspection Results_\" results from the [NYC Open Data](https://nycopendata.socrata.com/) website. \n",
    "\n",
    "* Click on the top \"1100+ Data Sets available\" and then search for the term \"_Restaurant Inspection Results_\".\n",
    "\n",
    "* If you see multiple data sets with the same title, pick the one with the most views.\n",
    "\n",
    "* Go to the data set and get the link for downloading the ZIP file. (It is under \"Export\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#your code here: download the file and store it under /home/ubuntu/data/restaurants.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note 1: This dataset is approximately 180Mb, so it can take 2-3 minutes to download; a URL to download a zipped version (~9.5Mb) is at https://dl.dropboxusercontent.com/u/16006464/IPDS/restaurant.zip\n",
    "\n",
    "Note 2: You need to unzip the restaurant.zip file to get the contents. (If needed, the `unzip` command can be installed using `sudo apt-get install unzip`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipes\n",
    "-----\n",
    "\n",
    "Pipes provide a way of connecting the output of one unix program or utility to the input of another, through standard input and output. \n",
    "\n",
    "Unix pipes give you the power to compose various utilities into a data flow and use your creativity to solve problems. Utilities are connected together (\"piped\" together) via the pipe operator, |. \n",
    "\n",
    "We will give more examples that use pipes later, after covering a few useful utilities first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filters\n",
    "-------\n",
    "\n",
    "### `cat`:\n",
    "\n",
    "Prints the contents of the specified files to standard output. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!curl -s -L 'https://dl.dropboxusercontent.com/u/16006464/IPDS/sample.txt' -o sample.txt #retrieve the file\n",
    "!cat sample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note_: The -L flag tells curl to follow \"redirects\" and -s tells curl not to print any output or statistics but rather store the file in the file specified by the -o flag.)\n",
    "\n",
    "If we also want to number the lines, we use the `-n` option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat -n sample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `less`:\n",
    "\n",
    "** _For use within the UNIX shell; not really useful to use it from iPython notebook_ **\n",
    "\n",
    "The command `cat` lets you see the contents of the file, but it is not convenient when the file is big. For that, it is better to use the command `less` which allows you to scroll and navigate through the contents of a file. When invoked like: `less [some big file]`. `less` enters an interactive mode. In this mode, several keys help you navigate the input file. Some key commands are:\n",
    "\n",
    "+ `(space)`: space navigates forward one screen.\n",
    "+ `(enter)`: enter navigates forward one line.\n",
    "+ `b`: navigates backwards one screen\n",
    "+ `y`: navigates backwards one line.\n",
    "+ `/[pattern]`: search forwards for the next occurrence of `[pattern]`\n",
    "+ `?[pattern]`: search backwards for the previous occurrence of `[pattern]`\n",
    "\n",
    "Where `[pattern]` can be a basic string or a regular expression. (We will cover regular expressions in the next session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `head/tail`:\n",
    "\n",
    "The `cat` command above lists the full file. This is problematic when dealing with big files, as it can often block the terminal or the iPython notebook. \n",
    "\n",
    "The `head` and `tail` commands can be used instead to output the first (last) lines of a file. Typically used like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Prints the first five lines of a file\n",
    "!head -n 5 sample.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Prints the last five lines of a file\n",
    "!tail -n 5 sample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The -n option specifies the number of lines to be output, the default value is 10. \n",
    "\n",
    "For more advanced usage, `tail`, when used with the `-f` option, will output the end of a file _as it is written to_. This is useful is a program is writing output or logging progress to a file, and you want to read it _live_ as it is happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `cut`:\n",
    "\n",
    "The `cat` command prints the full file. The `head` and `tail` commands select the first and last lines of a file. The `cut` command complements these commands by allowing us to select (or “cut”) certain fields (usually columns) from input. \n",
    "\n",
    "Cut is typically used with the `-f` option to specify a comma-separated list of columns to be emitted. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat sample.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Selects the first and fourth column. Assumes tab-separated columns.\n",
    "!cut -f1,4 sample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Specifying alternative delimeter instead \n",
    "\n",
    "`-d` option: To specify the string used to separate the fields use the `-d` option. For example, if spaces were used instead of tabs, we could change the above command to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Selects the second column, if the delimeter is space\n",
    "# Notice that we put space within quotes -d' '; alternatively we could use the escape character and write -d\\ \n",
    "# Notice that the only space characters appear between the words \"foo bar\", \"biz baz\", etc. The earlier separators are tabs.\n",
    "!cut -f2 -d' ' sample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "* Use the `cut` command to extract the restaurant names from the NYC Restaurant dataset. It is a comma separated file, so remember to specify correctly the separator. The restaurant name is the second column, and it is called \"DBA\" (doing business as)\n",
    "* Use the redirect operator > to save the outcome into a file named \"rest-names.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pipes \n",
    "\n",
    "Now, let's use pipes for the first time. Pipes are being used to \"pipe\" the output of one program into another. For example, select the second column, and then list only the first 3 entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cut -f 2 sample.txt | head -3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "* Select the restaurant names column, and then list the last 10 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sort`\n",
    "\n",
    "An extremely efficient implementation of [external merge sort](http://dzmitryhuba.blogspot.com/2010/08/external-merge-sort.html). In a nutshell, this means the sort utility can order a dataset far larger than can fit in a system’s main memory. While sorting extremely large files does drastically increase the runtime, smaller files are sorted quickly. Useful both as a component of larger shell scripts, and independently, as a tool to, say, quickly find the most active users, or to see the  most frequently loaded pages on a domain. \n",
    "\n",
    "Typically called like: `sort [options] [file]`. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat sample.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!sort sample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Some useful options:\n",
    "\n",
    "+ `-r`: reverse order. Sort the input in descending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!sort sample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ `-n`: numeric order. Sort the input in numerical order as opposed to the default lexicographical order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!sort -n sample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ `-k n`: sort the input according to the values in the n-th column. Useful for columnar  data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!sort -k 2 sample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `-t` the option to specify the text used to specify columns. Notice that `sort` uses the `-t` option to specify the delimiter character, while `cut` used `-d`. Yes, it is confusing. (This is mainly the result of developers creating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "* Sort the NYC Restaurant dataset by restaurant name and store the result in a separate file `/home/ubuntu/data/sorted.csv`. You will see that this is a comma separated file therefore the character that separates columns is the `,` (comma) character. The restaurant name is the second column in the dataset.\n",
    "* Repeat the sorting process above, but instead of storing the file, display the first 5 entries using the `head` command and a pipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#your code here: Get the restaurant names (the \"DBA\" column)\n",
    "#and store the result in a separate file /home/ubuntu/data/sorted.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#your code here: Use now the head command and a pipe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `uniq`\n",
    "\n",
    "Removes *sequential* duplicates: prints only those unique sequential lines from a file. For example, our sample.txt file contains a duplicate line at the end. See:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat sample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running `uniq` we can remove the duplicate line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!uniq sample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used with the `-c` option, uniq will report the number of duplicates of each line in the sequence. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!uniq -c sample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "For the following three exercises, perform them first by having multiple commands and storing them output of each command in a separate file. Then consolidate the process by using pipes.\n",
    "\n",
    "* Count the number of times that a restaurant name appears in the dataset. Use the `cut`, `sort`, `uniq` commands.\n",
    "* Count the number of times that a restaurant name appears in the dataset, and display in descending order of frequency the count and the restaurant name.\n",
    "* List the ten most frequent restaurant names, *without* displaying their frequency in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   4600 SUBWAY\r\n",
      "   3775 MCDONALD'S\r\n",
      "   2646 DUNKIN' DONUTS\r\n",
      "   2083 DUNKIN DONUTS\r\n",
      "   1906 STARBUCKS COFFEE\r\n",
      "   1500 CROWN FRIED CHICKEN\r\n",
      "   1482 KENNEDY FRIED CHICKEN\r\n",
      "   1413 BURGER KING\r\n",
      "   1294 DOMINO'S PIZZA\r\n",
      "    973 \"DUNKIN' DONUTS\r\n",
      "    850 CHIPOTLE MEXICAN GRILL\r\n",
      "    763 POPEYES CHICKEN & BISCUITS\r\n",
      "    668 GOLDEN KRUST CARIBBEAN BAKERY & GRILL\r\n",
      "    545 WENDY'S\r\n",
      "    523 AU BON PAIN\r\n",
      "    447 LITTLE CAESARS\r\n",
      "    444 CARVEL ICE CREAM\r\n",
      "    437 PAPA JOHN'S\r\n",
      "    435 IHOP\r\n",
      "    395 PRET A MANGER\r\n",
      "sort: write failed: standard output: Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "#your code here\n",
    "!cat /home/ubuntu/data/restaurants.csv | cut -f2 -d, | \\\n",
    "sort | uniq -c | sort -r -n | head -n 20 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `wc`: \n",
    "Compute word, line, and byte counts for specified files or output of other scripts. Particularly useful when used in concert with other utilities such as grep, sort, and uniq. Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\t1346699925\t11122\tfoo bar\r\n",
      "222\t1346699955\t11145\tbiz baz\r\n",
      "140\t1346710000\t11122\thee haw\r\n",
      "234\t1346700000\t11135\tbip bop\r\n",
      "146\t1346699999\t11123\tfoo bar\r\n",
      "99\t1346750000\t11135\tbip bop\r\n",
      "99\t1346750000\t11135\tbip bop\r\n"
     ]
    }
   ],
   "source": [
    "!cat sample.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7  35 201 sample.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc sample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicating the number of lines, words, and bytes in the file respectively. There are some useful flags for wc that will help you answer specific questions quickly:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ `-l`: get the number of lines from the input. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 sample.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l sample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ `-w`: get the number of words in the input. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 sample.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -w sample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ `-m`: the number of characters in the input. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201 sample.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -m sample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ `-c`: the number of bytes in the input. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201 sample.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -c sample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the number of bytes and characters are the same; all characters used are just one byte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "* List how many entries (inspections) there are in the restaurant data set\n",
    "* Remove any duplicate names\n",
    "* Report how many unique restaurants are in the dataset\n",
    "* \"Tricky\" questions: \n",
    "    * On average, how many words there are in a NYC restaurant name? Compute the answer both with, and without duplicate names. \n",
    "    * On average, how many characters in a NYC restaurant name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAMIS,DBA,BORO,BUILDING,STREET,ZIPCODE,PHONE,CUISINE DESCRIPTION,INSPECTION DATE,ACTION,VIOLATION CODE,VIOLATION DESCRIPTION,CRITICAL FLAG,SCORE,GRADE,GRADE DATE,RECORD DATE,INSPECTION TYPE\r\n",
      "30075445,MORRIS PARK BAKE SHOP,BRONX,1007      ,MORRIS PARK AVE                                   ,10462,7188924968,Bakery,03/03/2014,Violations were cited in the following area(s).,10F,\"Non-food contact surface improperly constructed. Unacceptable material used. Non-food contact surface or equipment improperly maintained and/or not properly sealed, raised, spaced or movable to allow accessibility for cleaning on all sides, above and underneath the unit.\",Not Critical,2,A,03/03/2014,01/14/2015,Cycle Inspection / Initial Inspection\r\n",
      "30075445,MORRIS PARK BAKE SHOP,BRONX,1007      ,MORRIS PARK AVE                                   ,10462,7188924968,Bakery,10/10/2013,No violations were recorded at the time of this inspection.,,,Not Applicable,,,,01/14/2015,Trans Fat / Second Compliance Inspection\r\n",
      "30075445,MORRIS PARK BAKE SHOP,BRONX,1007      ,MORRIS PARK AVE                                   ,10462,7188924968,Bakery,09/11/2013,Violations were cited in the following area(s).,04L,Evidence of mice or live mice present in facility's food and/or non-food areas.,Critical,6,A,09/11/2013,01/14/2015,Cycle Inspection / Re-inspection\r\n",
      "30075445,MORRIS PARK BAKE SHOP,BRONX,1007      ,MORRIS PARK AVE                                   ,10462,7188924968,Bakery,09/11/2013,Violations were cited in the following area(s).,04N,\"Filth flies or food/refuse/sewage-associated (FRSA) flies present in facility\u001as food and/or non-food areas. Filth flies include house flies, little house flies, blow flies, bottle flies and flesh flies. Food/refuse/sewage-associated flies include fruit flies, drain flies and Phorid flies.\",Critical,6,A,09/11/2013,01/14/2015,Cycle Inspection / Re-inspection\r\n",
      "30075445,MORRIS PARK BAKE SHOP,BRONX,1007      ,MORRIS PARK AVE                                   ,10462,7188924968,Bakery,08/14/2013,Violations were cited in the following area(s).,04C,Food worker does not use proper utensil to eliminate bare hand contact with food that will not receive adequate additional heat treatment.,Critical,32,,,01/14/2015,Cycle Inspection / Initial Inspection\r\n",
      "30075445,MORRIS PARK BAKE SHOP,BRONX,1007      ,MORRIS PARK AVE                                   ,10462,7188924968,Bakery,08/14/2013,Violations were cited in the following area(s).,04L,Evidence of mice or live mice present in facility's food and/or non-food areas.,Critical,32,,,01/14/2015,Cycle Inspection / Initial Inspection\r\n",
      "30075445,MORRIS PARK BAKE SHOP,BRONX,1007      ,MORRIS PARK AVE                                   ,10462,7188924968,Bakery,08/14/2013,Violations were cited in the following area(s).,06A,Personal cleanliness inadequate. Outer garment soiled with possible contaminant.  Effective hair restraint not worn in an area where food is prepared.,Critical,32,,,01/14/2015,Cycle Inspection / Initial Inspection\r\n",
      "30075445,MORRIS PARK BAKE SHOP,BRONX,1007      ,MORRIS PARK AVE                                   ,10462,7188924968,Bakery,08/14/2013,Violations were cited in the following area(s).,06C,\"Food not protected from potential source of contamination during storage, preparation, transportation, display or service.\",Critical,32,,,01/14/2015,Cycle Inspection / Initial Inspection\r\n",
      "30075445,MORRIS PARK BAKE SHOP,BRONX,1007      ,MORRIS PARK AVE                                   ,10462,7188924968,Bakery,08/14/2013,Violations were cited in the following area(s).,08A,Facility not vermin proof. Harborage or conditions conducive to attracting vermin to the premises and/or allowing vermin to exist.,Not Critical,32,,,01/14/2015,Cycle Inspection / Initial Inspection\r\n"
     ]
    }
   ],
   "source": [
    "!head /home/ubuntu/data/restaurant.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20583\r\n"
     ]
    }
   ],
   "source": [
    "#your code here\n",
    "!cat /home/ubuntu/data/restaurant.csv | cut -f2 -d, | sort | uniq | wc -l "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `find` \n",
    "\n",
    "Search directories for matching files. Useful when you know the name of a file (or part of the name), but do not know the file’s location in a directory. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!find ~ -name 'sample.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `grep`:\n",
    "A utility for pattern matching. grep is by far the most useful unix utility. While grep is conceptually very simple, an effective developer or data scientist will no doubt find themselves using grep dozens of times a day. grep is typically called like this: `grep [options] [pattern] [files]`. With no options specified, this simply looks for the specified pattern in the given files, printing to the console only those lines that match the given pattern. Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This in itself can be very useful, scraping large volumes of data to find what you’re looking for. \n",
    "\n",
    "The power of grep really shows when different command options are specified. Below are just a sample of the more useful grep options:\n",
    "\n",
    "+ `-v`: Inverted matching. In this setting, grep will return all the input lines that do not match the specified pattern. Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "executionInfo": null
   },
   "outputs": [],
   "source": [
    "!cat sample.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!grep -v 'biz baz' sample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ `-R`: Recursive matching. Here grep descends sub folders, applying the pattern on all files encountered. Very useful if you’re looking to see if any logs have lines that you’re interested in, or to find the source code file containing the function you’re interested in. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "executionInfo": null
   },
   "outputs": [],
   "source": [
    "!cd /home/ubuntu/data/; grep -R 'MORIMOTO' ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More options:\n",
    "\n",
    "* -c\tPrint only a count of matched lines.\n",
    "* -i \tIgnore lowercase and uppercase distinctions\n",
    "* -n\tPrint matching line with its line number\n",
    "* -v  \tNegate matches; print lines that do not match the regex\n",
    "* -r\tRecursively Search subdirectories listed\n",
    "* -l \tList only filenames\n",
    "* -o\tprints only the matching part of the line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will get back to grep once we learn regular expressions. You will see that grep can be extremely useful for searching through data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `jq`\n",
    "\n",
    "The [jq](http://stedolan.github.io/jq/) is not one of the \"standard\" UNIX tools but will be useful for us, to be able to parse the JSON responses of the Web API calls.\n",
    "\n",
    "Since it is not installed by default, we need to first install it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!sudo apt-get install jq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `jq` command has the format\n",
    "\n",
    "`jq [filters] filename`\n",
    "\n",
    "\n",
    "\n",
    "The absolute simplest (and least interesting) filter is `.` \n",
    "\n",
    "This is a filter that takes its input and produces it unchanged as output.\n",
    "\n",
    "Since jq by default \"pretty-prints\" all output, this trivial program can be a useful way of formatting JSON output from, say, curl.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!curl -s 'http://www.telize.com/geoip' > location.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m{\r\n",
      "  \u001b[0m\u001b[34;1m\"country_code3\"\u001b[0m\u001b[37m: \u001b[0m\u001b[32m\"USA\"\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"country\"\u001b[0m\u001b[37m: \u001b[0m\u001b[32m\"United States\"\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"offset\"\u001b[0m\u001b[37m: \u001b[0m\u001b[32m\"-4\"\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"country_code\"\u001b[0m\u001b[37m: \u001b[0m\u001b[32m\"US\"\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"latitude\"\u001b[0m\u001b[37m: \u001b[0m\u001b[0m39.0437\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"city\"\u001b[0m\u001b[37m: \u001b[0m\u001b[32m\"Ashburn\"\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"asn\"\u001b[0m\u001b[37m: \u001b[0m\u001b[32m\"AS14618\"\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"ip\"\u001b[0m\u001b[37m: \u001b[0m\u001b[32m\"54.174.159.22\"\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"dma_code\"\u001b[0m\u001b[37m: \u001b[0m\u001b[32m\"0\"\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"region_code\"\u001b[0m\u001b[37m: \u001b[0m\u001b[32m\"VA\"\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"isp\"\u001b[0m\u001b[37m: \u001b[0m\u001b[32m\"Amazon.com, Inc.\"\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"timezone\"\u001b[0m\u001b[37m: \u001b[0m\u001b[32m\"America/New_York\"\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"area_code\"\u001b[0m\u001b[37m: \u001b[0m\u001b[32m\"0\"\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"continent_code\"\u001b[0m\u001b[37m: \u001b[0m\u001b[32m\"NA\"\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"longitude\"\u001b[0m\u001b[37m: \u001b[0m\u001b[0m-77.4875\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"region\"\u001b[0m\u001b[37m: \u001b[0m\u001b[32m\"Virginia\"\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"postal_code\"\u001b[0m\u001b[37m: \u001b[0m\u001b[32m\"20147\"\u001b[0m\u001b[37m\r\n",
      "\u001b[37m}\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!jq . location.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having learned pipes, we can now avoid storing the output of curl into a file, and instead pass it directly through `jq`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m{\r\n",
      "  \u001b[0m\u001b[34;1m\"country_code3\"\u001b[0m\u001b[37m: \u001b[0m\u001b[32m\"USA\"\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"country\"\u001b[0m\u001b[37m: \u001b[0m\u001b[32m\"United States\"\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"offset\"\u001b[0m\u001b[37m: \u001b[0m\u001b[32m\"-4\"\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"country_code\"\u001b[0m\u001b[37m: \u001b[0m\u001b[32m\"US\"\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"latitude\"\u001b[0m\u001b[37m: \u001b[0m\u001b[0m39.0437\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"city\"\u001b[0m\u001b[37m: \u001b[0m\u001b[32m\"Ashburn\"\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"asn\"\u001b[0m\u001b[37m: \u001b[0m\u001b[32m\"AS14618\"\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"ip\"\u001b[0m\u001b[37m: \u001b[0m\u001b[32m\"54.174.159.22\"\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"dma_code\"\u001b[0m\u001b[37m: \u001b[0m\u001b[32m\"0\"\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"region_code\"\u001b[0m\u001b[37m: \u001b[0m\u001b[32m\"VA\"\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"isp\"\u001b[0m\u001b[37m: \u001b[0m\u001b[32m\"Amazon.com, Inc.\"\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"timezone\"\u001b[0m\u001b[37m: \u001b[0m\u001b[32m\"America/New_York\"\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"area_code\"\u001b[0m\u001b[37m: \u001b[0m\u001b[32m\"0\"\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"continent_code\"\u001b[0m\u001b[37m: \u001b[0m\u001b[32m\"NA\"\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"longitude\"\u001b[0m\u001b[37m: \u001b[0m\u001b[0m-77.4875\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"region\"\u001b[0m\u001b[37m: \u001b[0m\u001b[32m\"Virginia\"\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"postal_code\"\u001b[0m\u001b[37m: \u001b[0m\u001b[32m\"20147\"\u001b[0m\u001b[37m\r\n",
      "\u001b[37m}\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!curl -s 'http://www.telize.com/geoip' | jq . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The -s option for curl stands for \"silent\" and prevents the status messages from appearing in the output).\n",
    "\n",
    "The simplest useful filter is `.foo`. When given a JSON object as input, it produces the value at the attribute `foo`, or null if there’s none present.\n",
    "\n",
    "Now, let's try to use such a filter for selecting the \"city\" attribute listed in the JSON output: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\"Ashburn\"\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!curl -s 'http://www.telize.com/geoip' | jq '.city'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can also combine multiple attributes, using the addition operator `+`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\"Ashburn, Virginia, United States\"\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!curl -s 'http://www.telize.com/geoip' | jq '.city + \", \" + .region + \", \" + .country'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try something more complicated: We will use the jq command to read the location from the output of the ip-api.com API, and then create the URL for calling the OpenWeathermap API (see the previous session for details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\"http://api.openweathermap.org/data/2.5/weather?q=Ashburn,Virginia&mode=json&units=imperial\"\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!curl -s 'http://www.telize.com/geoip' | \\\n",
    "jq '\"http://api.openweathermap.org/data/2.5/weather?q=\" + .city + \",\" + .region + \"&mode=json&units=imperial\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!curl -s 'http://www.telize.com/geoip' | \\\n",
    "jq '\"http://api.openweathermap.org/data/2.5/weather?q=\" + .lat + \",\" + .region + \"&mode=json&units=imperial\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our first interaction with **variables**. You will notice that we used the `city` and `region` output from one service (Telize), in order to reuse these values later, in the OpenWeatherMap service.\n",
    "\n",
    "#### And now let's get advanced with pipes\n",
    "\n",
    "Now, we will get into a little more advanced topic. No worries if you feel lost.\n",
    "\n",
    "The key trick we will use is the `xargs` command. The `xargs` takes its input and passes it as a parameter to the command that follows.\n",
    "\n",
    "So, what we do below: \n",
    "* We first generate the URL, using the commands described above. \n",
    "* Then, we use the `xargs` command to pass the URL as a parameter to curl\n",
    "* Curl can then use this URL, and get the weather in our current location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m{\r\n",
      "  \u001b[0m\u001b[34;1m\"cod\"\u001b[0m\u001b[37m: \u001b[0m\u001b[0m200\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"name\"\u001b[0m\u001b[37m: \u001b[0m\u001b[32m\"Ashburn\"\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"id\"\u001b[0m\u001b[37m: \u001b[0m\u001b[0m4744870\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"sys\"\u001b[0m\u001b[37m: \u001b[0m\u001b[37m{\r\n",
      "    \u001b[0m\u001b[34;1m\"sunset\"\u001b[0m\u001b[37m: \u001b[0m\u001b[0m1442531664\u001b[0m\u001b[37m,\r\n",
      "    \u001b[0m\u001b[34;1m\"sunrise\"\u001b[0m\u001b[37m: \u001b[0m\u001b[0m1442487209\u001b[0m\u001b[37m,\r\n",
      "    \u001b[0m\u001b[34;1m\"country\"\u001b[0m\u001b[37m: \u001b[0m\u001b[32m\"US\"\u001b[0m\u001b[37m,\r\n",
      "    \u001b[0m\u001b[34;1m\"message\"\u001b[0m\u001b[37m: \u001b[0m\u001b[0m0.0109\u001b[0m\u001b[37m,\r\n",
      "    \u001b[0m\u001b[34;1m\"id\"\u001b[0m\u001b[37m: \u001b[0m\u001b[0m2856\u001b[0m\u001b[37m,\r\n",
      "    \u001b[0m\u001b[34;1m\"type\"\u001b[0m\u001b[37m: \u001b[0m\u001b[0m1\u001b[0m\u001b[37m\r\n",
      "  \u001b[37m}\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"coord\"\u001b[0m\u001b[37m: \u001b[0m\u001b[37m{\r\n",
      "    \u001b[0m\u001b[34;1m\"lat\"\u001b[0m\u001b[37m: \u001b[0m\u001b[0m39.04\u001b[0m\u001b[37m,\r\n",
      "    \u001b[0m\u001b[34;1m\"lon\"\u001b[0m\u001b[37m: \u001b[0m\u001b[0m-77.49\u001b[0m\u001b[37m\r\n",
      "  \u001b[37m}\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"weather\"\u001b[0m\u001b[37m: \u001b[0m\u001b[37m[\r\n",
      "    \u001b[37m{\r\n",
      "      \u001b[0m\u001b[34;1m\"icon\"\u001b[0m\u001b[37m: \u001b[0m\u001b[32m\"10d\"\u001b[0m\u001b[37m,\r\n",
      "      \u001b[0m\u001b[34;1m\"description\"\u001b[0m\u001b[37m: \u001b[0m\u001b[32m\"moderate rain\"\u001b[0m\u001b[37m,\r\n",
      "      \u001b[0m\u001b[34;1m\"main\"\u001b[0m\u001b[37m: \u001b[0m\u001b[32m\"Rain\"\u001b[0m\u001b[37m,\r\n",
      "      \u001b[0m\u001b[34;1m\"id\"\u001b[0m\u001b[37m: \u001b[0m\u001b[0m501\u001b[0m\u001b[37m\r\n",
      "    \u001b[37m}\u001b[0m\u001b[37m\r\n",
      "  \u001b[37m]\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"base\"\u001b[0m\u001b[37m: \u001b[0m\u001b[32m\"cmc stations\"\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"main\"\u001b[0m\u001b[37m: \u001b[0m\u001b[37m{\r\n",
      "    \u001b[0m\u001b[34;1m\"temp_max\"\u001b[0m\u001b[37m: \u001b[0m\u001b[0m84\u001b[0m\u001b[37m,\r\n",
      "    \u001b[0m\u001b[34;1m\"temp_min\"\u001b[0m\u001b[37m: \u001b[0m\u001b[0m73.4\u001b[0m\u001b[37m,\r\n",
      "    \u001b[0m\u001b[34;1m\"humidity\"\u001b[0m\u001b[37m: \u001b[0m\u001b[0m51\u001b[0m\u001b[37m,\r\n",
      "    \u001b[0m\u001b[34;1m\"pressure\"\u001b[0m\u001b[37m: \u001b[0m\u001b[0m1023\u001b[0m\u001b[37m,\r\n",
      "    \u001b[0m\u001b[34;1m\"temp\"\u001b[0m\u001b[37m: \u001b[0m\u001b[0m78.28\u001b[0m\u001b[37m\r\n",
      "  \u001b[37m}\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"wind\"\u001b[0m\u001b[37m: \u001b[0m\u001b[37m{\r\n",
      "    \u001b[0m\u001b[34;1m\"deg\"\u001b[0m\u001b[37m: \u001b[0m\u001b[0m199.002\u001b[0m\u001b[37m,\r\n",
      "    \u001b[0m\u001b[34;1m\"speed\"\u001b[0m\u001b[37m: \u001b[0m\u001b[0m4.45\u001b[0m\u001b[37m\r\n",
      "  \u001b[37m}\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"rain\"\u001b[0m\u001b[37m: \u001b[0m\u001b[37m{\r\n",
      "    \u001b[0m\u001b[34;1m\"1h\"\u001b[0m\u001b[37m: \u001b[0m\u001b[0m1.02\u001b[0m\u001b[37m\r\n",
      "  \u001b[37m}\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"clouds\"\u001b[0m\u001b[37m: \u001b[0m\u001b[37m{\r\n",
      "    \u001b[0m\u001b[34;1m\"all\"\u001b[0m\u001b[37m: \u001b[0m\u001b[0m1\u001b[0m\u001b[37m\r\n",
      "  \u001b[37m}\u001b[0m\u001b[37m,\r\n",
      "  \u001b[0m\u001b[34;1m\"dt\"\u001b[0m\u001b[37m: \u001b[0m\u001b[0m1442505426\u001b[0m\u001b[37m\r\n",
      "\u001b[37m}\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!curl -s 'http://www.telize.com/geoip' | \\\n",
    "jq '\"http://api.openweathermap.org/data/2.5/weather?q=\" + .city + \",\" + .region + \"&mode=json&units=imperial\"'  | \\\n",
    "xargs curl -s  | jq ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m78.28\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!curl -s 'http://www.telize.com/geoip' | \\\n",
    "jq '\"http://api.openweathermap.org/data/2.5/weather?q=\" + .city + \",\" + .region + \"&mode=json&units=imperial\"' | \\\n",
    "xargs curl -s | jq '.main.temp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Instead of pipes, we can also write and read files (although it is slower and more cumbersome, it may be useful while debugging):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!curl -s 'http://www.telize.com/geoip' > location.json\n",
    "!jq '\"http://api.openweathermap.org/data/2.5/weather?q=\" + .city + \",\" + .region + \"&mode=json&units=imperial\"' location.json > openweathermap.url\n",
    "!cat openweathermap.url | xargs curl -s > weather.json\n",
    "!jq '.main.temp' weather.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the power of pipes and filters: In three lines and in less than 200 characters, we created a service that reads out current location, using the API at ip-api.com, parses the output, creates a new API call for OpenWeatherMap, and then gets the data from that service, to give us back the temperature in our current location!\n",
    "\n",
    "The full manual of `jq` is available at http://stedolan.github.io/jq/manual/ and you can use the live demo at https://jqplay.org/\n",
    "\n",
    "There are numerous options in the manual. For now, you can restrict yourself to the basic operations that we covered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "* Instead of reading the city and region, read instead the log/lat coordinates from the Telize API, and modify the API call to OpenWeatherMap to use long/lat instead. (See http://openweathermap.org/current for the details API calls.)\n",
    "\n",
    "* Print the description of the weather, instead of the temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More examples of using pipes\n",
    "----------------------------\n",
    "\n",
    "We discussed earlier in the session the usage of the `|` operator to connect (aka \"pipe\") the output of one utility and direct it as input in another. Now that we have learned a few tools, lets use these in some examples. For instance, if you want to know how many records in the sample data file do not contain \"foo bar\", you can compose a data flow like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat sample.txt | grep -v 'foo bar' | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `wc` at the end of a pipe to count the number of matching output records is a common pattern. Recalling that `uniq` removes any sequential duplicates, we can count the number of unique users making purchases in our file by composing a data flow like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "executionInfo": null
   },
   "outputs": [],
   "source": [
    "!cat sample.txt | cut -f3 | sort | uniq  | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, if you want count how many transactions each user has appeared in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "executionInfo": null
   },
   "outputs": [],
   "source": [
    "!cat sample.txt | cut -f3 | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To now order the users by number of transactions made, you can try something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "executionInfo": null
   },
   "outputs": [],
   "source": [
    "!cat sample.txt | cut -f3 | sort | uniq -c | sort -nr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here, that the `-r` and `-n` flags for the sort command are combined. This is common shorthand and is acceptable for any unix utility."
   ]
  }
 ],
 "metadata": {
  "colabVersion": "0.1",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
