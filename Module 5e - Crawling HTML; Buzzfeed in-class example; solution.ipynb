{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Class Example: Crawl BuzzFeed\n",
    "\n",
    "* We will try to get the top articles that appear on Buzzfeed\n",
    "* We will grab the link for the article, the text of the title, the description, and the editor.\n",
    "* The results will be stored in a dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by parsing the first entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-8d251e11edd6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Let's get the first article and start parsing...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0marticle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marticleNodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mlink\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marticle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./header/div[@class='thumb-unit bf_dom']/a\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"href\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Link: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlink\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests # This command allows us to fetch URLs\n",
    "from lxml import html # This module will allow us to parse the returned HTML/XML\n",
    "\n",
    "url = \"http://www.buzzfeed.com/\"\n",
    "response = requests.get(url) # get the html of that url\n",
    "doc = html.fromstring(response.text) \n",
    "\n",
    "# After looking at the HTML, we find that each entry is within an \"article\" tag\n",
    "articleNodes  = doc.findall(\".//article\")\n",
    "# Let's get the first article and start parsing...\n",
    "\n",
    "article = articleNodes[0]\n",
    "link = article.find(\"./header/div[@class='thumb-unit bf_dom']/a\").get(\"href\")\n",
    "print \"Link: \" + link\n",
    "image = article.find(\"./header/div[@class='thumb-unit bf_dom']/a/div/img\").get(\"src\")\n",
    "print \"Image: \" + image\n",
    "titletext = article.find(\"./header/hgroup/h2/a\").text_content()\n",
    "print \"titletext: \" + titletext\n",
    "description = article.find(\"./p[@class='description ']\").text_content().strip()\n",
    "print \"description: \" + description\n",
    "editor = article.find(\"./p[@class='small-meta small-meta--full-width']/a[@class='username notranslate']\").text_content().strip()\n",
    "print \"editor: \" + editor\n",
    "responses = article.find(\"./p[@class='small-meta small-meta--full-width']//span[@class='num']\").text_content().strip()\n",
    "print \"responses: \" + responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's move the code for parsing a single article in its own function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-f88655fb8798>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-f88655fb8798>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    link = article.find(\"./header/div[@class='thumb-unit bf_dom']/a\").gethttp://www.washingtonpost.com/opinions/anti-abortion-advocates-problem-with-logic/2015/06/12/a3e08cc6-1108-11e5-9726-49d6fa26a8c6_story.html(\"href\")\u001b[0m\n\u001b[1;37m                                                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import requests # This command allows us to fetch URLs\n",
    "from lxml import html # This module will allow us to parse the returned HTML/XML\n",
    "\n",
    "def parse_article(article):\n",
    "    try:\n",
    "        link = article.find(\"./header/div[@class='thumb-unit bf_dom']/a\").gethttp://www.washingtonpost.com/opinions/anti-abortion-advocates-problem-with-logic/2015/06/12/a3e08cc6-1108-11e5-9726-49d6fa26a8c6_story.html(\"href\")\n",
    "        image = article.find(\"./header/div[@class='thumb-unit bf_dom']/a/div/img\").get(\"src\")\n",
    "        titletext = article.find(\"./header/hgroup/h2/a\").text_content()\n",
    "        description = article.find(\"./p[@class='description ']\").text_content().strip()\n",
    "        editor = article.find(\"./p[@class='small-meta small-meta--full-width']/a[@class='username notranslate']\").text_content().strip()\n",
    "        responses = article.find(\"./p[@class='small-meta small-meta--full-width']//span[@class='num']\").text_content().strip()\n",
    "        return (titletext, editor, description, responses, link, image)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "url = \"http://www.buzzfeed.com/\"\n",
    "response = requests.get(url) # get the html of that url\n",
    "doc = html.fromstring(response.text) \n",
    "\n",
    "articleNodes  = doc.findall(\".//article\")\n",
    "entries = [parse_article(articleNode) for articleNode in articleNodes]\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's put the code for creating a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Editor</th>\n",
       "      <th>Description</th>\n",
       "      <th>Responses</th>\n",
       "      <th>URL</th>\n",
       "      <th>ImageURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Title, Editor, Description, Responses, URL, ImageURL]\n",
       "Index: []"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests # This command allows us to fetch URLs\n",
    "from lxml import html # This module will allow us to parse the returned HTML/XML\n",
    "import pandas as pd\n",
    "\n",
    "def parse_article(article):\n",
    "    try:\n",
    "        link = article.find(\"./header/div[@class='thumb-unit bf_dom']/a\").get(\"href\")\n",
    "        image = article.find(\"./header/div[@class='thumb-unit bf_dom']/a/div/img\").get(\"src\")\n",
    "        titletext = article.find(\"./header/hgroup/h2/a\").text_content()\n",
    "        description = article.find(\"./p[@class='description ']\").text_content().strip()\n",
    "        editor = article.find(\"./p[@class='small-meta small-meta--full-width']/a[@class='username notranslate']\").text_content().strip()\n",
    "        responses = article.find(\"./p[@class='small-meta small-meta--full-width']//span[@class='num']\").text_content().strip()\n",
    "        return (titletext, editor, description, responses, link, image)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "url = \"http://www.buzzfeed.com/\"\n",
    "response = requests.get(url) # get the html of that url\n",
    "doc = html.fromstring(response.text) \n",
    "articleNodes  = doc.findall(\".//article\")\n",
    "data = [parse_article(articleNode) for articleNode in articleNodes if parse_article(articleNode) is not None]\n",
    "buzzfeedDataFrame = pd.DataFrame(data, columns=[\"Title\",\"Editor\",\"Description\",\"Responses\",\"URL\",\"ImageURL\"])\n",
    "\n",
    "buzzfeedDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
